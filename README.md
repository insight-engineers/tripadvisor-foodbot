# ðŸ½ï¸ TripAdvisor Foodbot

![python](https://img.shields.io/badge/python-3.11-blue?style=for-the-badge) ![package-manager](https://img.shields.io/badge/package_manager-uv-green?style=for-the-badge) ![GitHub last commit](https://img.shields.io/github/last-commit/insight-engineers/tripadvisor-foodbot?style=for-the-badge)

## ðŸ“ Overview

> [!NOTE]
> ðŸ“¦ Data for this project is sourced from repositories in the [`include`](include/) directory. It is scraped, processed, and cleaned using multiple modules. The `dm_tripadvisor` and `fs_tripadvisor` datasets are publicly available in BigQuery for authenticated users and power the restaurant recommendation system.

A next-generation restaurant recommendation system implementing **RAG (Retrieval-Augmented Generation)** architecture with **vector search** and **multi-criteria decision analysis (MCDA - ELECTRE III)**. Built on LlamaIndex for agent orchestration, Qdrant for vector storage, and BigQuery for data operations.

---

## ðŸš€ Core Components

- ðŸ” **Vector Search**: Qdrant with FastEmbed for dense retrieval
- ðŸ† **MCDA Engine**: ELECTRE III implementation for ranking restaurants
- ðŸ¤– **LLM Integration**: OpenAI API with streaming response to generate final natural responses
- ðŸ—„ï¸ **Data Layer**: BigQuery for structured data + Qdrant collections
- ðŸ§‘â€ðŸ’» **Agent Framework**: LlamaIndex with custom tools and callbacks

---

## âš™ï¸ Technical Implementation

### ðŸ§¬ Vector Search Pipeline

- âš¡ FastEmbed for dense embeddings generation
- ðŸ—ƒï¸ Qdrant collections for restaurant vectors
- ðŸ”— Hybrid search combining semantic and metadata filtering

### ðŸ… MCDA Implementation

- ðŸ§® **ELECTRE III** algorithm for restaurant ranking
- ðŸŽšï¸ Custom concordance/discordance thresholds
- ðŸ“Š Multi-criteria evaluation:
  - ðŸ² Food quality (delicious, fresh, etc.)
  - ðŸ’¸ Price sensitivity (affordable, expensive, etc.)
  - ðŸ›‹ï¸ Ambience (quiet, cozy, etc.)
  - ðŸ§‘â€ðŸ³ Service (friendly, fast, polite, etc.)
  - ðŸ“ Distance to user location (using GPS)
  - ðŸ”Ž Query matching (using `cosine similarity`)
  - ðŸ˜Š Review sentiment (positive, negative, etc.)

> [!IMPORTANT]
> âš¡ Because **ELECTRE III** is a **decision analysis** algorithm calculated based on lots of matrix operations, it can take significant time to rank restaurants with `numpy`. To improve performance and user experience, we use `numba` to speed up the ranking process by compiling the `numpy`-based functions with `njit` (`@njit` - alias for `@jit(nopython=True)`). This enables our Python functions to run at near-C speed.

---

### ðŸ“ Repository Structure

```bash
src/
â”œâ”€â”€ bigquery/    # BigQuery operations and data handlers
â”œâ”€â”€ chat/        # LlamaIndex agent implementation
â”œâ”€â”€ helper/      # Utility functions
â”œâ”€â”€ qdrant/      # Vector DB operations
â”œâ”€â”€ ranker/      # ELECTRE III implementation
â””â”€â”€ s3/          # S3 client for asset storage (user storage)
```

---

### ðŸ¤– Agent Implementation - `chat` Directory

- LlamaIndex RAG implementation
- Custom tools for:
  - ðŸ¥‡ `scoring_and_ranking`: generate candidate restaurants and rank them using MCDA
  - ðŸ“ `enrich_restaurant_recommendations`: enrich recommendations with more information and generate the final natural response
- ðŸ”„ Streaming response handlers for tool callbacks
- ðŸ§  Context management with chat history

---

## ðŸ› ï¸ Development Setup

### ðŸ“‹ Requirements

- ðŸ `python ~= 3.11`
- ðŸ“¦ `uv` package manager (for python deps)
- ðŸŒ `npx` (for `prisma` CLI)
- ðŸ”‘ OpenAI API key: [OpenAI Console](https://platform.openai.com/) (Mandatory)
- â˜ï¸ GCP account with BigQuery: [GCP Console](https://console.cloud.google.com/) (Optional, you can use local storage)
- ðŸŸ£ Qdrant Cloud instance: [Qdrant Console](https://cloud.qdrant.io/) (Optional, you can use my docker containers)
- ðŸ—„ï¸ AWS S3 (or alternative storage service using S3 API) (Optional, you can use my docker containers)

---

### âš¡ Quick Start

> [!NOTE]
> For local development, you can use the provided Docker containers for Qdrant and S3. If you want to use remote services, you can set up your own Qdrant and S3 instances or use the provided credentials in the `.env` file.
> - For remote services, if you want to use BigQuery, you must put the `sa.json` (service account JSON file) in the root directory and set the `FEATURE_STORAGE_MODE` to `remote` in the `src/helper/vars.py` file. If you want to use local storage, set it to `local`.
> - For Qdrant and S3, you can set the `QDRANT_*` and `*_AWS_*` environment variables in the `.env` file to your remote service credentials.
    
1. **Install dependencies with `uv`**

    ```bash
    uv sync
    ```

2. **Set up environment variables**

    Copy the `.env.example` file to `.env` and fill in the required values:

    ```bash
    cp .env.example .env
    ```

    Change the `OPENAI_API_KEY` to your OpenAI API key.

3. **Start containers (optional - ignore if using remote services)**

    If you want to use local Qdrant and S3, you can start the containers using `docker-compose`:

    ```bash
    docker compose up -d
    ```

    This will start Qdrant and S3-compatible storage services locally (MinIO). Make sure you have Docker installed and running. If you want to use remote services, you can skip this step.

4. **Initialize Qdrant collections**

    If you are using Qdrant, you need to initialize the collections. You can do this by running the following command:

    ```bash
    make load_qdrant
    ```

    This will create the necessary collections in Qdrant and load the initial data.

5. **Initialize Chatbot Schema (Prisma)**

    Because we are using Prisma for the storage of the chatbot schema, you need to run the following command to generate the Prisma client and apply the schema migrations:

    ```bash
    make db
    ```

    This will generate the Prisma client and apply the schema migrations.

    > Change the `DATABASE_URL` in the `.env` file to your database connection string if you are using a remote database.

6. **Start development server**

    ```bash
    make run
    ```

Check your `http://localhost:8000` for the LlamaIndex agent UI.

---

### ðŸ“¦ Dependencies

This project uses `uv` as the package manager. Dependencies are managed in the `uv` configuration file (`uv.yaml`).

- Core: `llama-index`, `fastembed`, `qdrant-client`
- Data: `google-cloud-bigquery`, `pandas`
- UI: `chainlit`

---

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---
